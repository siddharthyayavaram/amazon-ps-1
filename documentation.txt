Data Preprocessing Documentation

Step 1: Removing unnecessary columns 

The columns 'Customer Id' and 'Artist Name' are trivial and unique. They provide no value to the 
actual final shipping cost. 
Hence, these columns are dropped from the dataframe.

Step 2: Conversion of dates into appropriate usable format

Using the datetime module 'Scheduled Date' and 'Delivery Date' columns were converted to standard 
datetime format.
Example Conversion: 06/07/15 -> 2015-06-07

Since dates themselves do not provide significant value, we calculated the difference between the
'Scheduled Date' and 'Delivery Date' to gain more easily gained insight.
If delivery date is earlier than scheduled than the duration is positive and vice versa.
Example Conversion: Scheduled Date   Delivery Date   Difference 
                      2015-06-07	  2015-06-03         4

Step 3: Simpifying the customer location

Using the whole address makes it difficult to input the location into our regressor model. Further,
using specific pincodes and locations does not provide an overall insight and would be very difficult
to numerically encode it further down the line.

So the address string was split by spaces and only the second last was taken into account (index -> -2)
which represents the state within the address.
Example Conversion: New Michelle, OH 50777 -> OH 

Step 4: Filling in the missing values using KNNimputer.

Simple imputer is for 'Artist Reputation', 'Height'	,'Width'
KNNimputer is a scikit-learn class used to fill out or predict the missing values in a dataset. It is a 
more useful method which works on the basic approach of the KNN algorithm rather than the naive approach 
of filling all the values with mean or the median. It is based on Euclidean distance.
KNNimputer is for 'Weight' and 'Transport'

Step 5: Numerical encoding for non-numeric columns

One-hot encoding using the one-hot encoder from the sklearn library for non-numerical columns.
These include 'Material','International','Express Shipment', 'Installation Included','Transport','Fragile',
'Customer Information' and 'Remote Location'.
Used the function encoder.fit-transform.

Step 6: Removing outliers from a pandas DataFrame using the IQR method

It is important to remove ouliers or anomalies/exceptions from the data so that our model doesn't overfit 
to also take those outliers into account and hence doesn't skew our reults.

Using the IQR, the outlier data points are the ones falling below Q1–1.5 IQR or above Q3 + 1.5 IQR. 
The Q1 is the 25th percentile and Q3 is the 75th percentile of the dataset, and IQR represents the 
interquartile range calculated by Q3 minus Q1 (Q3–Q1
The IQR method of removing outliers was used on the following columns:
'Height'
'Width'
'Weight'
'Price Of Sculpture'

The total number of rows went down from 6500 rows to 4234 rows after data preprocessing.

Step 7: Removing extra features to fit in with our API of benchmark shipping cost.

'Artist Reputation','Material_Aluminium','Material_Brass','Material_Bronze','Material_Clay','Material_Marble',
'Material_Stone','Material_Wood','Installation Included_No','Installation Included_Yes','Customer Information_Wealthy',
'Customer Information_Working Class'

These features have a low correlation with the cost and hence can be removed without sacrificing much accuracy.



Machine Learning Documentation

1. Using LazyRegressor for an overall view

We used LazyRegressor from lazypredict's supervised models. LazyRegressor semi-automates the machine-learning task
lazypredict is a convenient wrapper library, that enables us to quickly fit all the models to our dataset and compare 
their performance so that the user can gain a good idea of which models are working the best for the given dataset though
hyperparameter tuning can also affect the results significantly.

Another important thing that is rather hidden from the user is that the library automatically applies simple preprocessing to the dataset.
First, it imputes missing values using SimpleImputer (using the mean for numeric features and a constant ‘missing’ value for categorical ones).
Then, it uses StandardScaler for numeric features and either OneHotEncoder or OrdinalEncoder for the categorical features (depending on the 
cardinality — number of unique values).

It provides Adjusted R-Squared, R-Squared, RMSE and time taken for each of the regressor used. From the table, we inferred that the following
models were working the best:
LGBMRegressor	
HistGradientBoostingRegressor
XGBRegressor
ExtraTreesRegressor
RandomForestRegressor
GradientBoostingRegressor

Apart from the above regressors, we also used baseline simple models such as linear regression.

2. Training our dataset on the above regressor models

We trained our models on the above regressor and found out that HistGradientBoostingRegressor was giving the best result with the best r-squared
value.

3. Preparing submission.csv file 

The sample submission format included 2 colums: 'Customer Id' and 'Cost'.
To prepare the submission file, we first stored the Customer Id column in a temp dataframe as it was needed later. The same steps for the pre-processing
were applied except removing the outliers as output was needed for all the rows.

We trained the test dataframe using HistGradientBoostingRegressor and created the submission.csv file.



